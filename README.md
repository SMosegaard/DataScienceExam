# Exam: Data Science, Prediction, and Forecasting

This repository contains the code for my exam project in the course "Data Science, Prediction, and Forecasting" as a part of my Master in Cognitive Science.

## 🔧 Setup and Installation Guide

Follow these steps to set up the project:

### 1.  Clone the Repository
Start by cloning the repository to your local machine using the following command:
```python
$ git clone "https://github.com/SMosegaard/DataScienceExam.git"
```

### 2. Navigate into the Repository
Navigate into the repository in your terminal:
```python
$ cd DataScienceExam
``` 

### 3. Set Up the Virtual Environment and Download the Data from Hugging Face
To get started, run the ```setup.sh``` script. The script will create a virtual environment, install all required dependencies specified in ```requirements.txt```, and download the datasets from Hugging Face.

*Note: The datasets used in this project are not created nor owned by me and therefore cannot be shared directly in this public repository. Thus, you will need to download the datasets (both the raw data and the preprocessed train/test splits) from Hugging Face.*

To access the data, please provide the token (--token / -t) provided in the exam paper. Run the following command:

```python
$ source setup.sh -t {private_token_provided_in_the_exam_paper}
``` 
Once the script completes, you will be working within the virtual environment and the datasets will be stored in the ```data/``` folder.

### 4. Prepare the Lag-Llama Environment 
Finally, navigate into the ```scr/``` folder and execute the bash script for the Lag-Llama model, that will clone the Lag-Llama documentation repository inside the folder:

```python
$ cd src
$ source lagllama_setup.sh
```

## 👩‍💻 Usage
Once the project has been set up, you can tune the hyperparameters for each model and test their performance. To do so, use the following commands and specify which model (--model / -m) to evaluate. There are currently three models available models: ARIMA, SVR, and Lag-Llama. Model names are not case-sensitive, so whether you type the options with capital letters or not will not affect the executions of the scripts.

```python
$ python tune.py -m {arima/svr/lagllama}
```
```python
$ python test.py -m {arima/svr/lagllama}
```
Please note, that the testing should only be performed after tuning, as ```test.py``` rely on optimized hyperparameters.

Based on the user input, the model will be tuned and tested on two datasets and two horizon lengths.

All outputs generated during tuning and testing is saved in the ```out/``` folder. The tuned hyperparameters are stored as files named ```{model}_hyperparameters.csv```, while the forecasting results are saved as ```{model}_{horizon}_{dataset}.csv```. Summaries of the evaluation metrics are also produced and saved as ```{model}_results.csv```

## 📈 Results

Finally, you can calculate the aggregated evaluation metrics and generate plots comparing the actual verus forecasted time series. To perform this step, simply run the following command:
```python
$ python results.py
```
The metrics summaries will be saved in the  ```out/``` folder as ```{model}_results_agg.csv```. Similarly, the generated visualizations will be saved in the ```plots/``` folder with filenames ```actual_vs_forecast_{dataset}_{horizon}.png```.

When finished, you can deactivate the virtual environment:
```python
$ deactivate
```

## 📂 Repository Overview
The project is structured to support a full forecasting workflow, including data prepreations, model tuning, evaluation, and visualization. Here is a breakdown of the project structure:

```
.
├── data/                          # Contains the datasets (when downloaded from Hugging Face) and a notebook for data visualisations
    ├── train                      # Training data
    ├── test                       # Testing data
    └── data_visualisations.ipynb  # Notebook for visualizing the data
├── out/                           # Stores output .csv files with tuned hyperparameters, forecast results, and metrics summaries  
├── plots/                         # Contains plots generated by the scripts in the src/ folder and the data notebook
├── src/                           # Source code for modeling and utilies
    ├── read_data.py               # Script for downloading the data from Hugging Face
    ├── lagllama_setup.sh          # Script for setting up the the Lag-Llama environment
    ├── utils.py                   # General-purpose helper functions
    ├── arima_utils.py             # Functions specific to ARIMA modeling
    ├── svr_utils.py               # Functions specific to SVR modeling
    ├── lagllama_utils.py          # Functions specific to Lag-Llama modeling
    ├── tune.py                    # Script for hyperparameter tuning across models
    ├── test.py                    # Script for testing and evaluating model performance
    └── results.py                 # Script for aggregating the retuls and plotting the forecasts
├── .gitignore 
├── README.md
├── requirements.txt               # Project dependencies
└── setup.sh                       # Script for setting up the project environment
```

## ⚠️ Notes
Python version: This project was developed and tested using Python version 3.10.0.

## 📍 References
For transparency, the hyperparameter tuning strategies are adapted from the following tutorials:
- [Lag-Llama tutorial](https://www.ibm.com/think/tutorials/lag-llama)
- [SVR tutorial 1](https://www.geeksforgeeks.org/time-series-forecasting-with-support-vector-regression/), [SVR tutorial 2](https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/)
